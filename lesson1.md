开始之前开redis-server.exe

***

***

***

> # 备注

大家好，我是高家华，很荣幸今天在这里和大家一起讨论python的性能分析

自我介绍：

- 来自腾讯反病毒实验室


- 12毕业加入腾讯


- 工作五年


- 一直从事客户端和后台的安全开发工作

## 课程介绍

这个系列课程的核心是讲Python的性能优化，在介绍这个课程之前，我想先和大家聊一下，python性能优化到底有什么用。

在硬件越来越便宜，速度越来越快的今天，如果自己写一个python小程序，或者说在个人电脑上跑的一个一次性python任务，那么性能优化并不重要，一个小程序10s或者100s跑完，区别不大；一次性的数据处理任务，即使性能不好，也没有太多性能分析的价值。等你性能分析，性能优化一套做完，这个任务都已经做完了，以后可能再也不做这个任务了，优化也就没有意义了。

但是真实的工作场景中，越来越多的地方使用python，机器学习（人工智能）现在的技术风口啊，这里边python用的很多。Scikit-learn，TensorFlow ，还有最近也比较火的量化交易，python也用的很多。

举个我们团队的例子，我们有几百台服务器组成的分析集群来进行木马病毒分析，集群调度和分析结果处理我们都是用python实现的，里边有一些机器学习模型的应用，也是python的。我们每天处理上千万的分析任务，python的性能优化就显得很有意义了，Python的性能优化做好了，既可以做到快速迭代，策略配置修改灵活，也可以做到海量任务处理。所以我个人觉得，Python性能优化还是比较重要的。

Python性能优化相关的课程会有三节

1. Python性能分析
2. Python性能优化的技巧及原理
3. Python性能优化实践

第一节主要讲怎么做性能分析，介绍一些性能分析的工具
第二节讲怎么做性能优化，怎么去写效率更高的python代码
第三节结合具体的例子讲性能优化实践
没有性能分析就无法进行性能优化，性能优化的技巧为解决实际问题服务，这三节课是个循序渐进的过程。



##背景知识
下边来说一下今天这节课适合的受众。整个这三堂课，属于python的初级偏中级的课程，首先需要写过一点python，没写过python直接听性能优化效果不会太好，对应不到已有知识点和技术点。大家也不用担心，听懂这个课也不需要对python多么精通，python不像C++学习曲线那么陡峭，毕竟只是一门脚本语言，边写边学就可以。如果在写Python的过程中思考过一些性能问题，或者现在的学习当中就有python性能优化的需求，那就比较适合听这门课了。
那什么样的人不适合呢？刚才说过没写过python不适合，下边我展开这节课的目录，大家看下，如果都了解，也不是目标受众，因为这堂课主要是讲利用工具进行性能分析，工具已经了解了再听就是浪费时间了。
## 目录介绍
今晚的课程分成三个部分，运行时间分析，内存分析，可视化工具。运行时间这部分介绍了三种简单分析方法，shell 的time命令，python的time模块，python的timeit模块，还有两个能做详细分析的性能分析器。第二部分内存分析，讲了两个内存使用情况分析工具，其中memory_profiler比较宏观，侧重点在整体的内存占用大小。objgraph比较微观，能得到每个对象的增长情况。第三部分介绍两个可视化的工具。如果上边提到的这些东西，你都比较熟悉，就像我刚才说的，你也不适合这个课程。现在可以关了这个视频了，开两局王者荣耀去吧。

## 概述：
下边正式开始今天的课程



- 什么是性能分析：
  分析代码和它正在使用的资源之间有着怎样的关系。例如，性能分析可以告诉你一个指令或者说一段代码占用了多少CPU时间，告诉你整个程序消耗了多少内存，告诉你这个代码片段增加分配了多少内存。

  性能分析是一个很大的话题，我们今天要讨论的是python性能分析。

  ​



- 怎么做python性能分析 

  + 程序运行的速度如何
  + 时间瓶颈在哪里/内存瓶颈在哪里
  + 改进性能瓶颈

  ​

> #正文

#1. 运行时间分析
- 算法时间复杂度

  说到运行时间分析，很多人会联想到算法时间复杂度，课堂上学的老师教的对比两种算法哪个更高效靠的就是这个。
  ​

  看这个表格，里边列了常见的算法时间复杂度。

  O(1)   消耗时间固定，不随着已有数据规模而增加，不涉及到n的概念

  先说第三个线性时间，回头再说对数时间。查找无序列表的最小元素，得把所有元素遍历一遍，这就是O(n)。像二分查找，就不需要所有的遍历一遍，比较一次砍掉一半数据，数据量越大越能体现出比线性时间的优势。

  线性对数时间，快速排序的理想情况，每次选取的标杆能把当前无序 列表平均分成两块，O(nlogn)

  冒泡排序，所有数据遍历一次是线性时间，每次要找到最小的或者最大的还是线性时间，O(n**2)

  ​

  算法时间复杂度是对算法做分析的基础，但我们今天不靠理论分析，就看实际测量的值。

## 1.1 Shell 命令time

  今天介绍的第一个方法，也是最简单的方法。
###安装
linux shell下的time命令。这个命令不仅仅用于python，可以直接测量一个shell命令的运行时间或者一个程序的运行时间，名字叫time但并不是用来显示和修改系统时间的。如果你使用的是Linux系统那么不需要任何安装操作，windows下如果要使用的话，要么是装虚拟机，要么通过是Cygwin这样的模拟环境，这里我推荐第二种。

###使用
 看下边这个图，ls 命令是linux下展示当前文件夹内容的命令。
  time命令直接测量ls命令的耗时，结果分三行，real是这个程序从开始到结束总共花了多少时间，user是cpu在用户态耗时，sys是cpu在内核态耗时。ls命令执行的比较快，下边我们换个执行的慢一点的。看这个python脚本，从0到9999做累加。time python 加脚本的名字，就能测量这个脚本的执行时间。这个脚本里边是累加操作，所以属于重计算的任务，再看下边这个脚本sleep 2s，这就不是重计算的任务，可以模拟一个重I/O的任务。

  ​

###小结

1. time命令首先要关注的就是real这个时间，这是程序性能最直观的展示

2. 其次要关注user和sys加起来作为一个整体和real的关系，real 不等于user + sys，第二个脚本就是为了说明这个问题，当程序中有I/O执行，或者等待一个事件的时候，real就会大于user+sys。real 和 user + sys的值越接近，证明程序越重计算，反之说明程序更重IO。

3. 有没有real 小于user + sys的情况呢，系统是多核并且代码优化的好有可能出现这种情况的。但这个不是重点，重点是前边说的两个。

   ​
##1.2 Python自带模块time

###安装
因为是Python自带的模块，不需要额外的安装操作，直接import进来就可以调用了。time模块的time函数返回的是一个绝对时间，1970早上8点0分0秒后经过的浮点秒数，asctime和localtime配合可以转成我们常见的时间格式。但是这里我们用这个模块来进行程序执行时间测量，所以asctime和localtime用不到。


###使用
一般情况进行时间测量任务，我们一开始会像下边这么写，两次计时取时间差。这么写确实简单，也足够灵活，两次测量之间加入Python代码和函数调用都很方便，但是当你的代码要分很多个小块，对每一个小块都去测量的时候，还按这么写就会比较麻烦，并且很不优雅。



一种比较流行的封装的方式，是timer.py这种写法。首先创建一个类，构造函数有个参数verbose，是个打印log的开关。然后有两个成员函数，__enter__和__exit__，如果python写的比较多就会知道__enter__和__exit__一般配合with关键字进行使用，enter这个函数进行第一次计时用start来存，exit这个函数进行第二次计时，并且做了一个减法计算，结果就是两次调用之间的时间间隔。



封装之后的使用在下边这个Python代码段中，首先把封装好的类import进来，第二行import的redis是一个nosql的数据库，挺热门的一个东西，这里不展开介绍了，如果你没听说就直接把它当成一个数据库就好。再下边一行代码，创建一个redis数据库，然后使用with关键字对数据存取分别使用我们封装的Timer进行操作计时。with关键字后边的这个Timer()就会自动调用enter，with所包含的语句块结束的时候会调用exit。这样就对with所包含的语块进行了时间测量。

###演示

**这个结果我没贴，实际带大家操作下**。（先启动redis server，然后再运行脚本）



顺便说一个time模块下的clock()函数，windows下推荐用time.clock代替time.time。

## 1.3 python模块timeit
###安装
Python有一个自己封装好的timeit模块，很类似刚才我们自己封装起来的timer.py。由于是python自带的模块，所以也不用额外的安装操作。

###使用
可以用timeit直接测一个Python语句的性能消耗，大家看我这里写的例子，就是直接测x = range(100)这句的性能消耗，看下边这个输出结果，大家觉得上边这行代码执行0.6s时间消耗高不高？

其实timeit默认就会循环1六个0，100万次。

如果不这么做的话，一行python代码的执行时间太短，计时器精度很难做到那么高。

###演示
刚才说了timeit很类似刚才我们封装起来timer.py，这里我带大家看下timeit的源码


## 1.4 Python默认性能分析器cProfile

上边讲的三种方法都比较简单，适合做粗略统计，下面讲两个性能分析器，相对于前边提到的shell 命令，time模块，timeit模块，性能分析器的分析结果更深入，更详细。


###安装
cProfile自Python 2.5以来就是标准版Python解释器默认的性能分析器，主要功能是测量CPU运行时间，统计函数调用次数。虽然没有针对内存的分析，但仍是性能优化过程中一个重要的环节，绝大多数时候这个都能为我们的分析工作提供有力支持。


###使用
cProfile有两种使用方式，一种是在Python代码中使用，import cProfile模块进来，另一种是在Python脚本启动的命令行中增加额外参数，针对整个脚本进行性能分析。

- 先说第一种情况，代码如下所示，
- 大家看一眼这块代码。代码讲解
- 运行结果比较长，没在我的文档中贴，我运行展示一下。

 ### 演示
```shell
ncalls：表示函数调用的次数；
tottime：表示指定函数的总的运行时间，除掉函数中调用子函数的运行时间；
percall：（第一个percall）等于 tottime/ncalls；
cumtime：表示该函数及其所有子函数的调用运行的时间，即函数开始调用到返回的时间；
percall：（第二个percall）即函数运行一次的平均时间，等于 cumtime/ncalls；
filename:lineno(function)：每个函数调用的具体信息；
**tips：原生（primitive）调用，表明这些调用不涉及递归**
```


- 下边说下在命令行中的使用，使用的Python脚本就是刚才在讲time模块的时候Redis的例子。

  直接运行一个脚本的使用时Python空格脚本路径，如果要使用cProfile的话就在中间增加 -m cProfile，注意cProfile要严格区分大小写的。

  ​

  第二行命令字 -o 这个参数后边跟的是输出的分析日志，这个分析日志是不能直接用记事本打开读的，不是把刚刚在命令行中直接打印出来的东西存到文本文件中，-o 命令生成的log要用专门的分析工具去打开，这个点我们在第三部分的可视化工具当中讲。如果单纯地想把命令行展示的东西存成文本文件，直接使用重定向符就好了。

  ​


##1.5 第三方性能分析器line_profiler
核心就在于line这个单词，这个性能分析器和cProfile不同。它可以帮助你一行一行地分析函数性能。cProfile主要关注函数的性能，如果你的程序性能瓶颈出现在某一行python代码中，line_profiler显得非常恰当。

cProfile是更深入的，每一行的代码的内部调用都被记录下，line_profiler只精确到行。

###安装
由于这是第三方的，需要我们安装下这个模块，python中安全第三方模块比较好用的是pip方式。


这个模块有部分是需要在本地系统中编译的，所以需要vs编译工具，如果你操作系统装了vs2008应该可以直接安装成功，要不然很可能会有个错误提示。

> Microsoft Visual C++ 9.0 is required 
> 如果你的电脑中装了比2008版本更高的vs，那么直接增加一个环境变量。
> 如果没有的话，去下边这个网址下载编译支持包，一路下一步，然后再用pip安装 。

###使用
使用方法，line_profiler的作者建议使用其中的kernprof工具，下边的介绍也是基于kernprof的。

首先修改源代码，在待测试函数的前边加上一行，@profile，对Python比较了解的人可能会知道，这个叫做装饰器，大概意思就是在函数外边再包一层，不知道也没关系，这不是今天必须要掌握的知识点。



往下看用来做测试的代码，这代码就是从0到9999逐个累加。在源代码上增加@profile这一行，保存，命令行启动 kernprof -l -v 两个参数，后边接待测试的脚本。-l 的意思是通知kerprof去分析被装饰的函数，-v的意思是执行完毕显示详细信息。



看下边的效果图，把待测函数的每一都列出来了，

- Line #：表示文件中的行号。


- Hits：性能分析时一行代码的执行次数。


- Time：一行代码执行的总时间，由计时器的单位决定。在分析结果的最开始有一行Timer unit

###演示
> **我这个图截取的有一点问题啊，少了这一行，我再运行一次**
>

该数值就是转换成秒的计时单位（要计算总时间，需要用Time数值乘以计时单位）。不同系统的计时单位可能不同。

- Per hit：执行一行代码的平均消耗时间，依然由系统的计时单位决定。


- % Time：执行一行代码的时间消耗占程序总消耗时间的比例。


# 2. 内存分析

到这里cpu运行时间的分析工具就讲完了，下边讲一下内存分析。

##2.1 内存占用变化memory_profiler

现在机器学习和深度学习很火热，很多学习任务比较吃内存，memory_profiler这种场景下可以起到一定作用。
###安装
memory_profiler也不是Python自带的模块，需要我们自己安装，这里仍然是推荐pip安装，第一步pip install memory_profiler，第二步pip install psutil（跨平台的系统监控模块，获取包括CPU、内存、磁盘、网络等信息），第二步windows上是必须的，linux上不用装。第三步pip install matplotlib，matplotlib是Python最著名的绘图库，它提供了一整套和matlab相似的命令API，十分适合交互式地进行制图。

windows上装matplotlib如果出现失败，先更新下pip，命令是pip install --upgrade pip。

###使用

memory_profiler的使用方法分为三种，第一种在命令行中直接使用。但是源码中也要在待测试的函数之上增加@profile这样一个装饰器，

看这个源代码，给大家10s

代码讲解，申请和释放。

然后命令行输入python -m memory_profiler memory_profiler_test.py。



效果如下图所示，这个输出结果和上边介绍的line_profiler非常像，都是把行当成分析目标的。line_profiler是展示每行的执行时间，这个是展示内存占用的变化。第一列是行号，第二列是执行到当前行整个脚本占用的总内存，第三列代表当前行代码的执行带来的内存变化，后边就是源代码了。

很方便帮我们定位哪部分代码带来大量的内存开销。



第二种使用方法是在python代码中使用，还是刚才的代码增加这么一行from memory_profiler import profile，就不用在执行的时候增加额外的命令行参数了，但是效果是一样的，所以不详细介绍了。



下边重点说一下第三种用法，memory_profiler装完会生成一个mprof，其实也是一个python脚本，只不过没有以.py结尾。

mprof的作用是是输出基于时间的内存测量，上边两种方式都是输出基于代码行的测量。使用方法是python mprof run 后边跟要运行的脚本，运行完之后再执行一次python mprof plot，就会展示出内存随时间变化的折线图。
###演示
下面我给大家演示一下。





## 2.1 微观分析objgraph

### 安装
这个模块也需要安装后才能使用，pip可以直接安装

### Python的内存问题

 > 首先明确一个点，python中的内存问题相比于C和C++少很多。
 >
 > C或者C++内存管理由开发者负责，Python中内存管理是由Python解释器负责，所以开发人员从内存事务中解脱出来，使得错误更少，程序更健壮，开发周期更短。

提到了python自动内存管理，就提一下python的主要垃圾回收算法

- 引用计数

- 标记清除

- 分代回收

  ​

Python不会出现C或者C++那种令人发指，难以定位的那种例如堆破坏的内存问题。

python可能出现的内存问题：
(1)所用到的用 C 语言开发的底层模块中出现了内存问题，这种情况不是我们今天需要关心的

(2)代码中用到了全局的 list、 dict 或其它容器，不停的往这些容器中插入对象，而忘记了在使用完之后进行删除回收，有点类似内存泄漏的感觉。

我们来看下边的代码，pdb这个模块是调试用的，set_trace()之后就能接管python的执行流程。

>  代码讲解
>
>  类，构造函数申请内存
>
>  computate_something函数默认参数，函数内部一共生成了三个MyBigFatObject对象。

### 演示

在调试的过程中，给大家展示objgraph的用法。



#3. 可视化工具
## 3.1 Runsnakerun

### 安装

由于Runsnakerun需要依赖GUI界面库wxpython，所以先装这个GUI库。

​	wxpython不适合使用pip安装，可以去官网下载安装包，注意要区分64位和32位版本，否则无法正常运行runsankerun。32位系统只能装32位版本，64位系统建议大家装64的python，wxpython库也装64位的，免得出莫名其妙的问题。

​	wxpython装好后，可以用pip装runsnakerun

​	runsnakerun安装完毕会生成一个runsnake.py

### 使用

python  runsnake.py  result.prf

runsnake.py 的输入不是一个Python脚本，而是性能分析log，这个log就是我们之前介绍cProfile的时候用-o生成的log。

### 演示

​	这个是有GUI界面的，演示下会比较清楚

1. 标签介绍
2. 排序
3. 右侧图形
4. function/location
5. Percent
6. 跟踪





  

## 3.2 pycallgraph

### 安装

依赖graphviz，网上下载安装包http://www.graphviz.org/，安装完成后把安装路径的bin文件夹路径添加到环境变量中。

然后pip安装pycallgraph，pip install pycallgraph

### 使用

  pycallgraph在python脚本中使用，看下边代码，和我们之前自己封装的那么timer类似，使用了with关键字来对待测代码和函数进行性能测试。

  ​

  我这里只是展示一下用法，如果在实际使用中，不要去改待测试脚本，把待测试脚本import进来，单独写一个测试脚本。

### 演示

  ```
  python pycallgraph graphviz -- python_time_test0.py
  ```

  ​